---
title: "MSFT Malware - H2O Deep Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Carregando bibliotecas

```{r}
library(h2o)
library(dplyr)
library(stringr)
library(readr)
library(tidyr)
library(ggplot2)
library(ggrepel)
library(knitr)
library(ggthemes)

```

# Conectando ao cluster H2O

```{r}
seed <- 1974

h2o.connect()

```

# Carregando arquivo train

```{r}
arquivo <- file.path("/home/cluster/kaggle/data/train.csv")

df <- h2o.importFile(path = arquivo)

```

# ggplot theme

```{r}
my_theme <- function(base_size = 12, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
    theme(
      axis.text = element_text(size = 12),
      axis.title = element_text(size = 14),
      panel.grid.major = element_line(color = "grey"),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "aliceblue"),
      strip.background = element_rect(fill = "darkgrey", color = "grey", size = 1),
      strip.text = element_text(face = "bold", size = 12, color = "white"),
      legend.position = "right",
      legend.justification = "top", 
      panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
    )
}
```

# Explorando as variáveis

```{r}
info <- c("Label", "Type", "Missing", "Zeros", "Cardinality")

kable((train_described <- h2o.describe(df)[info]) %>% arrange(Type))

```

```{r, eval=FALSE}
h2o.describe(df) %>%
  gather(x, y, Zeros:Sigma) %>%
  mutate(group = ifelse(x %in% c("Min", "Max", "Mean"), "min, mean, max", 
                        ifelse(x %in% c("NegInf", "PosInf"), "Inf", "sigma, zeros"))) %>% # separating them into facets makes them easier to see
  mutate(Label = factor(Label, levels = colnames(df))) %>%
  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +
  geom_point(size = 4, alpha = 0.6) +
  scale_color_brewer(palette = "Set1") +
  my_theme() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  facet_grid(group ~ ., scales = "free") +
  labs(x = "Feature",
       y = "Value",
       color = "")

```

# Visão geral sobre os dados do dataset train

```{r}
h2o.summary(df) 
```

# Explorando as variáveis

```{r}
numeric_cols <- h2o.columns_by_type(df, coltype = "numeric")
string_cols <- h2o.columns_by_type(df, coltype = "string")
categorical_cols <- h2o.columns_by_type(df, coltype = "categorical")
```

## Variáveis Numéricas

```{r}
variaveis_numericas <- data.frame(variavel = h2o.colnames(df[numeric_cols]), stringsAsFactors = FALSE)

for (i in 1:nrow(variaveis_numericas)) {
  variaveis_numericas[i, "variacao"] <- h2o.var(df[variaveis_numericas[i, "variavel"]],
                                                na.rm = TRUE)

  variaveis_numericas[i, "cardinalidade"] <- h2o.nrow(h2o.unique(df[variaveis_numericas[i, "variavel"]]))
  
  variaveis_numericas[i, "numero_NAs"] <- h2o.nacnt(df[variaveis_numericas[i, "variavel"]])
}
  
kable(variaveis_numericas %>% arrange(desc(variacao)))

```

## Variáveis Texto

```{r}

variaveis_texto <- data.frame(variavel = h2o.colnames(df[string_cols]),
                               stringsAsFactors = FALSE)

for (i in 1:nrow(variaveis_texto)) {
  variaveis_texto[i, "numero_NAs"] <- h2o.nacnt(df[variaveis_texto[i, "variavel"]])
}
  
kable(variaveis_texto)


```

## Variáveis Categóricas

```{r}
lista_categoricas <- data.frame(variavel = h2o.colnames(df[categorical_cols]), stringsAsFactors = FALSE)

for (i in 1:nrow(lista_categoricas)) {
  lista_categoricas[i, "variacao"] <- h2o.var(df[lista_categoricas[i, "variavel"]], na.rm = TRUE)
  lista_categoricas[i, "cardinalidade"] <- h2o.nrow(h2o.unique(df[lista_categoricas[i, "variavel"]]))
  lista_categoricas[i, "numero_NAs"] <- h2o.nacnt(df[lista_categoricas[i, "variavel"]])
}
  
kable(lista_categoricas %>% arrange(desc(cardinalidade)))

```

## Variável HasDetections

### Transformando em factor

```{r}
df$HasDetections <- as.factor(df$HasDetections)
```

### Gráfico

```{r}
h2o.table(df$HasDetections) %>%
  as.data.frame() %>%
  ggplot(aes(HasDetections, Count)) +
  geom_bar(stat = "identity", alpha = 0.7, (aes(fill = HasDetections))) +
  theme_stata() + scale_color_stata() 

```

## Variável HasOpticalDiskDrive

### Tranformando em factor (categorical)

```{r}
df$Census_HasOpticalDiskDrive <- as.factor(df$Census_HasOpticalDiskDrive)
```

```{r}
h2o.table(df$Census_HasOpticalDiskDrive) %>%
  as.data.frame() %>%
  ggplot(aes(Census_HasOpticalDiskDrive, Count)) +
  geom_bar(stat = "identity", alpha = 0.7, (aes(fill = Census_HasOpticalDiskDrive))) +
  theme_stata() + scale_color_stata() 

# versão do IE - quanto maior melhor (mais atualizado)?
h2o.unique(df$IeVerIdentifier) # 303 versões diferentes -> cardinalidade alta
df$IeVerIdentifier <- as.factor(df$IeVerIdentifier)

h2o.unique(df$Firewall) # Valores 0, 1 e NA
h2o.nacnt(df$Firewall) # 91350 NAs
df$Firewall <- as.factor(df$Firewall)

h2o.unique(df$IsBeta)
h2o.nacnt(df$IsBeta)
df$IsBeta <- as.factor(df$IsBeta)
h2o.table(df$IsBeta) # Praticamente valor constante...
h2o.var(df$IsBeta, na.rm = TRUE)

h2o.unique(df$HasTpm)
df$HasTpm <- as.factor(df$HasTpm)

h2o.unique(df$IsProtected)
h2o.nacnt(df$IsProtected) # 36044 NAs
df$IsProtected <- as.factor(df$IsProtected)
h2o.var(df$IsProtected, na.rm = TRUE)
h2o.table(df$IsProtected)

h2o.table(df$IsProtected) %>% 
  as.data.frame() %>%
  ggplot(aes(IsProtected, Count)) +
  geom_bar(stat = "identity")

h2o.unique(df$IsSxsPassiveMode)
df$IsSxsPassiveMode <- as.factor(df$IsSxsPassiveMode)
h2o.table(df$IsSxsPassiveMode) # Praticamente valor constante...
h2o.var(df$IsSxsPassiveMode, na.rm = TRUE)

as.data.frame(h2o.table(df$IsSxsPassiveMode)) %>% 
  ggplot(aes(IsSxsPassiveMode, Count)) +
  geom_bar(stat = "identity")

h2o.unique(df$AVProductsInstalled) # baixa cardinalidade
df$AVProductsInstalled <- as.factor(df$AVProductsInstalled)
h2o.table(df$AVProductsInstalled)

as.data.frame(h2o.table(df$AVProductsInstalled)) %>% 
  ggplot(aes(AVProductsInstalled, Count)) +
  geom_bar(stat = "identity")

h2o.unique(df$AVProductsEnabled) # baixa cardinalidade
df$AVProductsEnabled <- as.factor(df$AVProductsEnabled)
h2o.table(df$AVProductsEnabled)

# Grande maioria com apenas 1 antivirus habilitado
as.data.frame(h2o.table(df$AVProductsEnabled)) %>% 
  ggplot(aes(AVProductsEnabled, Count)) +
  geom_bar(stat = "identity")

```

```{r}

```


identificadores <- h2o.colnames(df)[str_which(h2o.colnames(df), "Identifier")]

identificadores

response <- "HasDetections"

variables_to_remove <- c("MachineIdentifier", 
                         "ProductName", 
                         "PuaMode", 
                         "IsBeta", 
                         "Census_IsWIMBootEnabled",
                         "DefaultBrowserIdentifier", # grande numero de NA's
                         "OsBuidLab", # cardinalidade muito alta
                         "",
                         "Census_OSVersion" # cardinalidade muito alta
                          )


#predictors <- setdiff(predictors, c(variables_to_remove, identificadores))
predictors <- setdiff(h2o.colnames(df), c(variables_to_remove,response))

df_split <- h2o.splitFrame(df, ratios = c(0.6, 0.2))

train <- df_split[[1]]
validation <- df_split[[2]]
test <- df_split[[3]]

##################################################
# Deep Learning H2O - Default
##################################################

dl_default_model <-  h2o.deeplearning(model_id = "dl_default_model",
                                      x = predictors, 
                                      y = response,
                                      training_frame = train,
                                      validation_frame = validation,
                                      epochs = 2)

h2o.saveModel(dl_default_model, path = "/home/cluster/kaggle/results/")

#dl_default_model <- h2o.loadModel(path = "/home/cluster/kaggle/results/dl_default_model")

# Model Performance

summary(dl_default_model)

# só com cv
#h2o.mean_per_class_error(dl_default_model, train = TRUE, valid = TRUE, xval = TRUE)

h2o.confusionMatrix(dl_default_model, valid = TRUE)

plot(dl_default_model,
     timestep = "epochs",
     metric = "classification_error")

plot(dl_default_model,
     timestep = "samples",
     metric = "classification_error")

# quanto menor o logloss, melhor o modelo
plot(dl_default_model,
     timestep = "epochs",
     metric = "logloss")

# quanto mais próximo de zero, melhor
plot(dl_default_model,
     timestep = "epochs",
     metric = "rmse")

# auc -> quanto mais perto de 1, melhor
h2o.auc(dl_default_model, train = TRUE) # 0.7018412
h2o.auc(dl_default_model, valid = TRUE) # 0.6983061
#h2o.auc(dl_default_model, xval = TRUE) # só vale quando usar cv

# Importancia das variaveis
h2o.varimp(dl_default_model)

h2o.varimp_plot(dl_default_model)

perf <- h2o.performance(dl_default_model, test)
perf

# MSE:  0.2189903
# RMSE:  0.467964
# LogLoss:  0.6261572
# Mean Per-Class Error:  0.4056845
# AUC:  0.6979393
# pr_auc:  0.7050555
# Gini:  0.3958785

plot(perf)

##################################################
# Deep Learning H2O - Stopping method = AUC
##################################################

dl_auc_stopping_model <-  h2o.deeplearning(model_id = "dl_auc_stopping_model",
                                      x = predictors, 
                                      y = response,
                                      training_frame = train,
                                      validation_frame = validation,
                                      stopping_rounds = 5,
                                      stopping_metric = "AUC",
                                      stopping_tolerance = 0.001,
                                      activation = "RectifierWithDropout",
                                      score_each_iteration = TRUE,
                                      hidden = c(200, 200, 200, 200, 200),
                                      epochs = 5,
                                      variable_importances = TRUE,
                                      export_weights_and_biases = TRUE
                                      )

h2o.saveModel(dl_auc_stopping_model, path = "/home/cluster/kaggle/results/")

# Model Performance

summary(dl_auc_stopping_model)

h2o.mean_per_class_error(dl_auc_stopping_model, train = TRUE, valid = TRUE, xval = TRUE)

h2o.confusionMatrix(dl_auc_stopping_model, valid = TRUE)

plot(dl_auc_stoppping_model,
     timestep = "epochs",
     metric = "classification_error")

plot(dl_auc_stopping_model,
     timestep = "samples",
     metric = "classification_error")

# quanto menor o logloss, melhor o modelo
plot(dl_auc_stopping_model,
     timestep = "epochs",
     metric = "logloss")

# quanto mais próximo de zero, melhor
plot(dl_auc_stopping_model,
     timestep = "epochs",
     metric = "rmse")

# auc -> quanto mais perto de 1, melhor
h2o.auc(dl_auc_stopping_model, train = TRUE)
h2o.auc(dl_auc_stopping_model, valid = TRUE)
h2o.auc(dl_auc_stopping_model, xval = TRUE)

# Importancia das variaveis
h2o.varimp(dl_auc_stopping_model)

perf <- h2o.performance(dl_auc_stopping_model, test)
perf

plot(perf)

##################################################

h2o.shutdown(prompt = FALSE)

